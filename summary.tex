\documentclass{scrartcl}
\usepackage{setspace}
\begin{document}
\onehalfspacing
\obeylines
\setlength{\parindent}{0pt}
\begin{center}\LARGE\textbf{Advanced Natural Language Processing}\end{center}

\section{LLMs}
\subsection*{BERT}
Bidirectional Encoder Representations from Transformers.
Basically a transformer encoder.
Training: BERT was trained on Books and Wikipedia.
Training objectives: Masked Language Modelling. Predict randomly masked tokens (by [MASK] token). Next Sentence Prediction (with [CLS] token).
Versions: 
- BERT-Base: 12 transformer blocks, embedding dim: 768, 12 attention heads. 
- BERT-Large: 24 transformer blocks, embedding dim: 1024, 16 attention heads.
Extensions:
- RoBERTa: Larger batches, removed NSP.
- SentenceBERT: BERT + siamese architecture (Sentence A and B in the same model, outputs are compared by some metric) + triplet loss (tune network such that distance between anchor sentence and positive sentence is smaller than anchor sentence and negative sentence)
- DistillBERT: Larger teacher network creates soft probability distribution labels. Smaller student model tries to replicate these distributions.

\subsection*{GPT}
Generative Pretrained Transformer.
Only a transformer decoder, since its task is not seq2seq, where you might want to refer to another part of the input sentence, but instead just generate next tokens (still the decoder receives already generated tokens as inputs).
Goal is to use few-shot learning and prompting instead of fine-tuning and task specific architectures.
Autoregressive: Feeds back generated tokens in decoder input.

\subsection*{Other LLMs}
\subsubsection*{T5}
Text-to-Text-Transfer-Transformer.
Encoder and decoder block, similar to original Attention is all you need paper but with slight modifications.

\subsubsection*{RETRO}
Retrieval-Enhanced Transformer.
Enhanced by large text database.
In total 4\% of GPT-3 size.

\subsection*{Flash Attention}
Self-attention is the bottleneck when training/inferencing transformers (quadratic time).
During Attention, tensors are moved from HBM (Hidh Bandwith (GPU) Memory) to SRAM (Shared random-access (GPU) memory) multiple times: Before each operation, the tensors are moved from HBM to SRAM and then written back to HBM.
Flash Attention fuses multiple operations into one kernel, thus saving load and write operations. It is becoming the standard for transformers.

\subsection*{Fine-Tuning}
Traditional fine-tuning: Re-train all parameters with a small labelled dataset for example on domain knowledge.
Traditional fine-tuning with freezing: Freeze a part of the model.

\subsection*{Prompting}
Removes the need for additional layers for a downstream task. Uses the same format as the pretraining objective of LLMs: We ask the LLM to generate the answer based on a task-specific input.
Requires no new parameter, can be practiced on a closed-source LLM.

\subsubsection*{Zero-Shot}
Simple prompting is equivalent to zero-shot learning. The model predicts the answer based on the question.
Example: "Translate English to French: cheese = ".

\subsubsection*{In-Context Learning and Few-Shot Learning}
Learning that contains examples of solutions to the task.
Example: "Translate English to French: sea otter = loutre de mer. cheese = ".

\subsubsection*{Prompting Terminology}

Pattern:
A function that maps input to text (i.e. template for $x$).
Example: $f(x) = "Review: x"$

Verbalizer:
A function that maps a label to text (i.e. template for $y$).
Example: $v(y) = "Sentiment: y"$

Zero-shot prompting is using one pattern. Few-shot prompting is using many pairs of patterns and verbalizers and one final pattern.
Choosing a prompt is important and non-trivial, since experiments show different patterns and verbalizers exhibit large variance on the result accurarcy. Additionaly, it is uncertain how and why in-context learning works.
Reasons:
- Different input and output space distributions decrease/increase performance.
- LLM sees demonstrations not as ordered pairs.
- Highly dependent on choice, order and term frequency.
Prompting still works really well in practice and can perform better than smaller task-specific fine tuned models such as BERT.

\subsection*{Parameter Efficient Fine-Tuning}

\subsubsection*{Prompt-based Fine Tuning}
Prompt-based fine-tuning = prompt + gradient updates.
Instead of fine-tuning and predicting a [CLS] token for an input sentence, use the masked prompt as input sentence.
Classical fine-tuning: [CLS] A three-hour cinema master class.
Prompt-based fine tuning: [CLS] A \_ master class. It was [MASK].

\subsubsection*{Prompt Search}




\end{document}